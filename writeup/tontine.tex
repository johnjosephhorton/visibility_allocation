\section{An Allocation Mechanism for Web-Scale Assignment Problems} 
Algorithm \ref{alg:find-equitable}, or the Equitable Criterion (EP)
algorithm shows that meeting the exact proportion criterion is
possible in some situations and gives a method of constructing the
necessary bistochastic matrix. Further, by meeting the exact proportion
criterion, it gives us continuity in merit. However, in the platform
applications that motivated the problem, this approach is likely to
prove infeasible: we cannot solve an LP every time an individual's
budget changes or even make draws from the bistochastic matrix.

An algorithm we can use in applications should at least offer
continuity and monotonicity in merit, and ideally exact proportion. It should be
easy to incorporate changes in $\vec{b}$ or $\vec{v}$ and deliver
assignments as they are needed, in the order that they are likely to
be needed, i.e., we can learn the first position before the second
position and the second position before the third, and so on.

One other desirable feature for the algorithm---a feature not usually
seen as an important attribute---is conceptual simplicity, as it will
make it easier for the individuals subject to the control of the
algorithm to understand and hopefully support as just the resultant
allocation. Distributive justice, which is what the algorithm is
attempting to deliver, is only one kind of justice---there is also
procedural justice \cite{rawls1999theory}, which is concerned with the
transparency and fairness of the process. Being able to clearly
explain to platform participants why they are appearing in search in
way they can understand would be a potentially powerful advantage.

\subsection{Reverse Tontine Mechanism}
\label{sec:tontine}
We propose a simple approach we call the ``Reverse Tontine" (RT) that
is nearly identical to the simple serial dictatorship, except that the
dictator $i$ at each step is chosen with probability
$\frac{b_i}{\sum_j b_j}$, where $j$ indexes all the unallocated
individuals in that step. The name was chosen because in this
mechanism, it is desirable to ``die'' early (unlike an actual
tontine), and when you die, your probability shares are distributed to
``survivors.''

\begin{algorithm} 
\caption{Reverse Tontine (RT). This algorithm takes as input vectors
  $\vec{b}$ of individual budgets shares and $\vec{v}$ of values. Both
  vectors are of the same length. The algorithm returns an assignment
  of individuals to values.}
\label{alg:tontine}

\begin{algorithmic}
\WHILE{there are unassigned individuals} 
\STATE Draw an individual $i$ with probability equal to their budget share,
$b_i \left(\sum_j b_j\right)^{-1}$. 
\STATE Assign individual $i$ to position $\max \vec
v$. 
\STATE Remove the drawn individual from $\vec b$ and the selected position
from $\vec v$ 
\ENDWHILE
\end{algorithmic}
\end{algorithm} 


Given the description of the Algorithm~\ref{alg:tontine}, one might
worry that it would require $n-k$ re-normalizations of the budgets at
step $k$, which combined with the steps needed to actually make the
draws might make this approach computational
unattractive. Fortunately, there is a simple and fast algorithm that
requires no re-weighting and can generate draws with $n$ individuals
in $\log_2 n$ steps. To do this, we first make the $n$ individuals
the terminal nodes in a binary tree. We combine pairs of nodes, giving
the parent node a value equal to the sum of the values of its
children. Let $T$ be our resultant binary tree, which consists of
nodes $t \in T$. The right and left children of a non-terminal node
$t$ are $r(t)$ and $l(t)$, respectively. Each node has a value, $|t|$:
if the node is terminal, then $|t_j| = b_j$, otherwise $|t_j| =
|l(t_j)| + |r(T_j)|$. If a node is missing children, its value is just
the value of its remaining child (if any).

To sample an individual, we start at the root node, $t_0$. It has a
value equal to the sum of all individual budgets. We draw a random
number $\gamma \sim U[0,1]$. If $\gamma < \frac{|l(t_0)|}{|l(t_0)| +
  |r(t_1)|}$, we move to the left child node; otherwise we move to the
right. We then repeat the same procedure, using the values of the new
nodes children. Once we reach a terminal node, we have the individual
assigned to that position. We complete the process by moving back up
the tree, ``repairing'' the values for each tree to reflect that the
selected individual is no longer available for allocation.

\begin{proposition}
The binary tree draw method selects individuals proportional to their
budget share.
\end{proposition} 
\begin{proof}  
Let $b_i$ be the budget of a particular individual. They are selected
iff a unique series of moves down the tree occurs, $m_1,m_2,\ldots
m_k$ that generate a sequence of visited nodes $t_0t_1,t_2,\ldots t_k$
(where $t_0$ is the root). The probability of this sequence occurring
(and thus $b_i$ being selected) is:
\begin{eqnarray*} 
  Pr(b_i) = \frac{|t_1|}{|l(t_0)| + |r(t_0)|} \times
  \frac{|t_2|}{|l(t_1)| + |r(t_1)|} \times \ldots \times
  \frac{b_i}{|l(t_{k-1})| + |r(t_{k-1})|}
\end{eqnarray*} 
Because $|t_k| = |r(t_k)| + |l(t_k)|$, we can cancel all numerators
and denominators except for the first denominator and the last
numerator. This gives us $Pr(b_i) = \frac{b_i}{|l(t_0)| + |r(t_0)|}$,
and since $|t_0| = \sum_j b_j$, $Pr(b_i) = \frac{b_i}{\sum_j b_j}$.
\end{proof} 

%\begin{prop} 
%The computational complexity is $O(n \log n)$ and the memory
%complexity is $O(n)$.
%\end{prop} 
%\begin{proof}
%  TK.
%\end{proof} 

\begin{proposition}
An individual's expected outcome from the RT mechanism is continuous
in their budget share.
\end{proposition}
\begin{proof} 
  
  Let us first define notation for the expected value of an individual
  with budget $b_i$ when there are $n$ other individuals as $x_n(b_i,
  \vec{b}, \vec{v})$. We can proceed by induction. First, when $n=2$,
  under the RT mechanism, with $b_1 > b_2$ and $v_1 > v_2$, we have
  $x_2(b_1, \vec{b}, \vec{v}) = b_1v_1 + b_2v_2$ and $x_2(b_2,
  \vec{b}, \vec{v}) = b_1v_1 + b_2v_2$. Both expected values are
  linear functions of the respective budgets and hence
  continuous. Now, we assume that $x_m(.)$ is continuous and consider
  the $m+1$ case in which we add a new position $v'$ and a new
  individual with budget $b'$.

With a probability equal to his or her budget share, $b_i$ is selected
immediately and receives $v^* = \max v \cup v'$. Let us denote that
probability as $s_{m+1}(i) = \frac{b_i}{\sum_{b \in \vec{b} \cup b'}
  b}$, which is continuous in $b_i$. If the individual $i$ does not
get selected in that round, some other individual $k$ is selected,
with probability $s_{m+1}(k)$. When that $k$ individual is selected,
they remove $v^*$ from consideration and remove their budget,
$b_k$. For each of the $k$ scenarios this generates, this leaves the
individual $i$ facing an $m$-sized RT scenario. We can write the
expected value recursively, as:


\begin{equation*} 
x_{m+1}(b_i, \vec{b} \cup b', \vec{v} \cup v') = 
s_{m+1}(i) v^* + 
\left(1- s_{m+1}(i) \right) 
\sum_{k=1}^{m} s_{m+1}(k) x_m (b_i, \vec{b} \cup b' \setminus b_k, \vec{v} \cup v' \setminus v^*) 
\end{equation*}


We can see that $x_{m+1}()$ is a finite linear combination of
continuous functions (recall that $x_{m}(.)$ is continuous by
assumption) and hence is itself continuous.
\end{proof} 

\begin{proposition} 
  The reverse tontine mechanism does not always satisfy the exact
  proportion criterion.
\end{proposition} 
\begin{proof}
We can show this with a simple counter-example with $n=2$. The
equitable allocation for $b_1 > b_2$ (with $b_1 + b_2 = 1$ and $v_1 >
v_2$ is $\mathbf{E}[u_1] = b_1 \left(v_1 + v_2\right)$ while the
weighted serial approach gives: $\mathbf{E}[u_1] = b_1 v_1 + b_2
v_2$. For the two expectations to be equal, $b_1 = b_2$.
\end{proof} 

\begin{proposition} \label{prop:fosd}
The distribution of outcomes for an individual first order
stochastically dominates (FOSD) the distribution of outcomes for any
other individual with a lower budget, facing the same collection of
other individuals.
\end{proposition}
% http://en.wikipedia.org/wiki/Stochastic_dominance
\begin{proof}
  Assume that we have two individuals with budgets $b_h$ and $b_l$
  respectively, with $b_h > b_l$. We want to compare the
  distributions of the two individuals under RT. First, let us define
  $S_m(b)$ as the expected sum of the budgets for the individuals
  after $m$ individuals have already been selected, not including
  individual with budget $b$. Choose any position $k$ such that $1 \le
  k < n$.  We are interested in the probability that the individual
  received a position $q \le k$ (and hence of higher value), which we
  can think of as a cdf, $Pr(q \le k) = F(k; b)$. We can write the
  probability of an individual being selected after $k$ as a function
  of their budgets. For $b_h$, we have: 
  \begin{equation*}
    1 - F(k; b_l) = \left(1-b_l\right)\left(1-\frac{b_l}{S_1(b_l)}\right)
    \ldots \left(1 - \frac{b_l}{S_k(b_l)} \right) 
    \end{equation*} 
whereas for $b_l$, we have: 
     \begin{equation*}
    1 - F(k; b_h) = \left(1-b_h\right)\left(1-\frac{b_h}{S_1(b_h)}\right)
    \ldots \left(1 - \frac{b_h}{S_k(b_h)} \right) 
    \end{equation*} 
     
Because $b_h > b_l$ and by the assumption that both the high and low
individuals face the same collection of individuals, for all $k$,
$S_k(b_h) = S_k(b_l)$, and hence $(1-b_h S_m(b_h)^{-1}) < (1-b_l
S_m(b)^{-1})$, which means that every term of $1-F(k; b_h)$ is less
than every term of $1-F(k; b_k)$ and thus the distribution of outcomes
for $b_h$ FOSD $b_l$.
\end{proof} 

A direct implication of Proposition \ref{prop:fosd} is that any
utility maximizer, regardless of attitude towards risk, would prefer
to have a higher budget to a lower budget.

